---
title: "Datathon-2019"
author: "Mihir Dutta, Bhrij Patel, and Kevin Day"
output: pdf_document
---

```{r setup, include=FALSE}
library(readr)
library(randomForest)
library(tidyverse)
library(dplyr)
library(reshape)
library(skimr)
```

## Read Data

```{r}
training = read_csv("training_tallskinny.csv")
validation = read_csv("validation_tallskinny.csv")
interest_topics = read_csv("interest_topics.csv")

training_filtered <- training %>%
  filter(!is.na(ltiFeatures)) %>%
  filter(!is.na(stiFeatures))
validation_filtered <- validation %>%
  filter(!is.na(ltiFeatures)) %>%
  filter(!is.na(stiFeatures))

interest_topics$topic_name <- as.character(interest_topics$topic_name)
category <- vector(mode="character", length=1411)
for (i in 1:1411) {
  category[i] = strsplit(interest_topics[i,2], '/')[[1]][2]
} 
new_interest_topics <- cbind(interest_topics,category)

training_filtered <- right_join(training_filtered,new_interest_topics, by = "topic_id")
  
sums <- training_filtered %>%
  group_by(userID,category) %>%
  summarise(lti = sum(ltiFeatures), sti = sum(stiFeatures))

categories <- sort(unique(new_table$a))



sums_lti <- sums[1:3]
sums_sti <- sums[c(1,2,4)]

lti <- cast(sums_lti, userID~category)
sti <- cast(sums_sti, userID~category)

lti[is.na(lti)] <- 0
sti[is.na(sti)] <- 0

inAudience <- training_filtered[1:2]
inAudience <- inAudience %>%
  group_by(userID) %>%
  distinct()

sti_2 <- left_join(sti, inAudience, by = "userID")
sti_2 <- sti_2[-which(sti_2$userID==0),]

lti_2 <- left_join(lti, inAudience, by = "userID")
lti_2 <- lti_2[-which(lti_2$userID==0),]

sti_2 <- sti_2 %>%
  mutate(inAudience = recode_factor(inAudience, "True" = 1, "False" = 0))

lti_2 <- lti_2 %>%
  mutate(inAudience = recode_factor(inAudience, "True" = 1, "False" = 0))

columns <- colnames(lti_2)
columns <- gsub(" & ", "_",columns)
columns <- gsub(" ", "_",columns)
colnames(lti_2) <- columns
colnames(sti_2) <- columns



lti <- lti_2
sti <- sti_2

lti_valid <- read_csv("lti_valid.csv")

```

## Data Cleaning 

```{r}
combined <- left_join(sti, lti, by = "userID")

sti$inAudience <- as.character(sti$inAudience)
sti$inAudience <- as.factor(sti$inAudience)

lti$inAudience <- as.character(lti$inAudience)
lti$inAudience <- as.factor(lti$inAudience)

lti = subset(lti, select = -c(X1))


columns <- colnames(lti)
columns <- gsub(" & ", "_",columns)
columns <- gsub(" ", "_",columns)
colnames(lti) <- columns


lti_valid = subset(lti_valid, select = -c(X1))

lti_valid$inAudience <- as.character(lti_valid$inAudience)
lti_valid$inAudience <- as.factor(lti_valid$inAudience)

columns <- colnames(lti_valid)
columns <- gsub(" & ", "_",columns)
columns <- gsub(" ", "_",columns)
colnames(lti_valid) <- columns
```

## Undersample to Balance Classes

```{r} 
set.seed(1234)
even_sample <- function(dataset, n){
  zero_lti <- which(dataset$inAudience == 0)
  one_lti <- which(dataset$inAudience == 1)

  nsamp <- n 

  pick_zero_lti <- sample(zero_lti, nsamp)
  pick_one_lti <- sample(one_lti, nsamp)
  
  sample_d <- lti[c(pick_zero_lti, pick_one_lti), ]
  
  return(sample_d)
}

even_lti <- even_sample(lti, 100)
```

## Test Error Calc Function

```{r}
mtry_error <- function(mtry_test) {
  test_model <- randomForest(inAudience ~ . -userID, data=even_lti, mtry=mtry_test, importance=TRUE, na.action=na.omit) #test on lti
  
  test_sample <- even_sample(lti_valid, 100)
  test_sample_resp <- test_sample$inAudience

  test_pred <- predict(test_model, test_sample) #predict one lti_valid

  test_tab <- table(test_pred, test_sample_resp)

  test_mce <- 1-sum(diag(test_tab))/sum(test_tab) 
  return(test_mce)
}
```

## Hyperparameter Tuning

```{r}
set.seed(1234)

mce_vec <- vector()

for (i in 3:20) {
  mtry_sum = 0
  for (j in 1:10) {
    mtry_sum = mtry_sum + mtry_error(j)
  }
  mtry_mean = mtry_sum/10
  mce_vec = c(mce_vec,mtry_mean)
}

#mtry = 13
```
## Calculate Accuracy

```{r}
set.seed(5678)

random_forest_model <- randomForest(inAudience ~ . -userID, data=even_lti, mtry=13, importance=TRUE, na.action=na.omit)


lti_sample <- even_sample(lti_valid, 100)
lti_sample_resp <- lti_sample$inAudience

lti_pred <- predict(random_forest_model, lti_sample)

tab <- table(lti_pred, lti_sample_resp)

mce <- 1-sum(diag(tab))/sum(tab) 
```

```{r}
accuracy = 1 - mce
print(paste('Accuracy',accuracy))


p00 <- as.numeric(tab[1, 1])
p01 <- as.numeric(tab[1, 2])
p10 <- as.numeric(tab[2, 1])
p11 <- as.numeric(tab[2, 2])

con_mat <- matrix(c(p00/(p00+p01),p01/(p00+p01),p10/(p10+p11),p11/(p10+p11)), ncol=2)
colnames(con_mat) <- c('Observed Zero', 'Observed One')
rownames(con_mat) <- c('Predicted Zero', 'Predicted One')
confusion_matrix <- as.table(con_mat)

confusion_matrix
```

## Feature Importance

```{r}
feat_imp <- importance(random_forest_model, type = 1)

feat_imp %>%
  sort(decreasing = TRUE)
```

